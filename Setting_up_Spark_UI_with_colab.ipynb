{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12tPj96afgq5-iKwCKkaHD58Ccw5aWDo1",
      "authorship_tag": "ABX9TyPLQD7uYOdSRErIIFOz42Zt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/infyprakash/bigdata/blob/main/Setting_up_Spark_UI_with_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Spark UI with colab"
      ],
      "metadata": {
        "id": "xf9cPNEaad6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "install system dependencies"
      ],
      "metadata": {
        "id": "BJySZnFxahWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "16hepaOOLo9R"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "_LptAqJiL92o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the environment for Spark."
      ],
      "metadata": {
        "id": "eQfz3eouazOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "d9jBGd-wMeBA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6bPugbDMjkl",
        "outputId": "e684ae44-7921-41a1-8cdf-f65d472fb891"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "rFz7x7ZEMnom"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "ovOt_SkyMryn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n"
      ],
      "metadata": {
        "id": "rfE1ngADMtWa"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "IQIj_g9WM8Fe",
        "outputId": "68fa8010-f162-4027-bbdd-d949fd31ae7b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f19de56c0d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b513d4386989:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbnP1HSZNCSO",
        "outputId": "164be3c4-40ce-489f-9b6b-1b3ec1313ca3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-02 17:30:17--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 18.205.222.128, 54.161.241.46, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.2’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  8.15MB/s    in 1.6s    \n",
            "\n",
            "2023-03-02 17:30:19 (8.15 MB/s) - ‘ngrok-stable-linux-amd64.zip.2’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "add your authtoken, by visiting this link: https://dashboard.ngrok.com/get-started/your-authtoken"
      ],
      "metadata": {
        "id": "UgntfOnSYrRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok authtoken 2MSyp2URPtWLIMoxKXmARsQ6bLv_3hKEipAvCyhgbska5vwhi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vkAut8wU2Sl",
        "outputId": "6d0233d5-3e6b-49c6-acf5-6622ceefdacd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./ngrok start --all --authtoken \"2MSyp2URPtWLIMoxKXmARsQ6bLv_3hKEipAvCyhgbska5vwhi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALiq6GmkV1TT",
        "outputId": "3c08926f-9c4d-48d0-b78f-4522aa170ee3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME:\n",
            "   start - start tunnels by name from the configuration file\n",
            "\n",
            "USAGE:\n",
            "   ngrok start [command options] [arguments...]\n",
            "\n",
            "DESCRIPTION:\n",
            "   Starts tunnels by name from the configuration file. You may specify any\n",
            "   number of tunnel names. You may start all tunnels in the configuration\n",
            "   file with the --all switch.\n",
            "\n",
            "EXAMPLES:\n",
            "   ngrok start dev        # start tunnel named 'dev' in the configuration file\n",
            "   ngrok start web blog   # start tunnels named 'web' and 'blog'\n",
            "   ngrok start --all      # start all tunnels defined in the config file\n",
            "\n",
            "OPTIONS:\n",
            "   --all\t\tstart all tunnels in the configuration file\n",
            "   --authtoken \t\tngrok.com authtoken identifying a user\n",
            "   --config\t\tpath to config files; they are merged if multiple\n",
            "   --log \"false\"\tpath to log file, 'stdout', 'stderr' or 'false'\n",
            "   --log-format \"term\"\tlog record format: 'term', 'logfmt', 'json'\n",
            "   --log-level \"info\"\tlogging level\n",
            "   --none\t\tstart running no tunnels\n",
            "   --region \t\tngrok server region [us, eu, au, ap, sa, jp, in] (default: us)\n",
            "\n",
            "ERROR:  Your configuration file must define at least one tunnel when using --all. To intentionally start no tunnels, use `ngrok start --none.`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "visit the given public url to access web UI"
      ],
      "metadata": {
        "id": "Q3pGVhrHa4MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:4040/api/tunnels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iiC32gAV_fT",
        "outputId": "d720d972-9c7c-438b-e7c9-685e5ed1616d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"tunnels\":[{\"name\":\"command_line (http)\",\"uri\":\"/api/tunnels/command_line%20%28http%29\",\"public_url\":\"http://feb6-35-243-247-246.ngrok.io\",\"proto\":\"http\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":0,\"gauge\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0},\"http\":{\"count\":0,\"rate1\":0,\"rate5\":0,\"rate15\":0,\"p50\":0,\"p90\":0,\"p95\":0,\"p99\":0}}},{\"name\":\"command_line\",\"uri\":\"/api/tunnels/command_line\",\"public_url\":\"https://feb6-35-243-247-246.ngrok.io\",\"proto\":\"https\",\"config\":{\"addr\":\"http://localhost:4050\",\"inspect\":true},\"metrics\":{\"conns\":{\"count\":1,\"gauge\":0,\"rate1\":0.011458136074669095,\"rate5\":0.0030925140804588976,\"rate15\":0.0010836790736504383,\"p50\":30004388177,\"p90\":30004388177,\"p95\":30004388177,\"p99\":30004388177},\"http\":{\"count\":1,\"rate1\":0.0069497108324461715,\"rate5\":0.0027982224558022787,\"rate15\":0.001048151847790129,\"p50\":3127472,\"p90\":3127472,\"p95\":3127472,\"p99\":3127472}}}],\"uri\":\"/api/tunnels\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True) # Property used to format output tables better\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "-dd90OyOXij3",
        "outputId": "1854124f-3715-4cfb-8b33-fd882004e633"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f19de56c0d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b513d4386989:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the dataset"
      ],
      "metadata": {
        "id": "UaSpCUhDddGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDJ5G6TQdUWs",
        "outputId": "87290691-d533-40e9-b6d4-31d92cccf337"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-02 17:54:40--  https://jacobceles.github.io/knowledge_repo/colab_and_pyspark/cars.csv\n",
            "Resolving jacobceles.github.io (jacobceles.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to jacobceles.github.io (jacobceles.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/cars.csv [following]\n",
            "--2023-03-02 17:54:40--  https://jacobcelestine.com/knowledge_repo/colab_and_pyspark/cars.csv\n",
            "Resolving jacobcelestine.com (jacobcelestine.com)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to jacobcelestine.com (jacobcelestine.com)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22608 (22K) [text/csv]\n",
            "Saving to: ‘cars.csv’\n",
            "\n",
            "\rcars.csv              0%[                    ]       0  --.-KB/s               \rcars.csv            100%[===================>]  22.08K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-03-02 17:54:40 (18.7 MB/s) - ‘cars.csv’ saved [22608/22608]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESxwechxdjNd",
        "outputId": "eb0b2a50-bbd9-4cc5-dbdb-4be004f49bac"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cars.csv  ngrok-stable-linux-amd64.zip\t  sample_data\n",
            "drive\t  ngrok-stable-linux-amd64.zip.1  spark-3.1.1-bin-hadoop3.2\n",
            "ngrok\t  ngrok-stable-linux-amd64.zip.2  spark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"cars.csv\",header=True,sep=\";\")"
      ],
      "metadata": {
        "id": "xVWlpAp7dlpW"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy9SZ4JSd0U8",
        "outputId": "766b6072-69b1-4a3b-a372-e15c5ef9ef86"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "|Chevrolet Chevell...|18.0|        8|       307.0|     130.0| 3504.|        12.0|   70|    US|\n",
            "|   Buick Skylark 320|15.0|        8|       350.0|     165.0| 3693.|        11.5|   70|    US|\n",
            "|  Plymouth Satellite|18.0|        8|       318.0|     150.0| 3436.|        11.0|   70|    US|\n",
            "|       AMC Rebel SST|16.0|        8|       304.0|     150.0| 3433.|        12.0|   70|    US|\n",
            "|         Ford Torino|17.0|        8|       302.0|     140.0| 3449.|        10.5|   70|    US|\n",
            "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "sPTSNokmd64E",
        "outputId": "d33d7200-9438-43be-99f2-ff81ec2ef093"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-------+--------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------+\n",
              "|summary|                 Car|               MPG|        Cylinders|      Displacement|       Horsepower|            Weight|      Acceleration|             Model|Origin|\n",
              "+-------+--------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------+\n",
              "|  count|                 406|               406|              406|               406|              406|               406|               406|               406|   406|\n",
              "|   mean|                null|23.051231527093602|5.475369458128079| 194.7795566502463|103.5295566502463|2979.4137931034484|15.519704433497521| 75.92118226600985|  null|\n",
              "| stddev|                null|   8.4017773522706|1.712159631548529|104.92245837948867|40.52065912106347| 847.0043282393513|2.8033588163425462|3.7487373454558743|  null|\n",
              "|    min|AMC Ambassador Br...|                 0|                3|             100.0|                0|             1613.|              10.0|                70|Europe|\n",
              "|    max|        Volvo Diesel|               9.0|                8|             98.00|            98.00|             5140.|              9.50|                82|    US|\n",
              "+-------+--------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+------------------+------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>summary</th><th>Car</th><th>MPG</th><th>Cylinders</th><th>Displacement</th><th>Horsepower</th><th>Weight</th><th>Acceleration</th><th>Model</th><th>Origin</th></tr>\n",
              "<tr><td>count</td><td>406</td><td>406</td><td>406</td><td>406</td><td>406</td><td>406</td><td>406</td><td>406</td><td>406</td></tr>\n",
              "<tr><td>mean</td><td>null</td><td>23.051231527093602</td><td>5.475369458128079</td><td>194.7795566502463</td><td>103.5295566502463</td><td>2979.4137931034484</td><td>15.519704433497521</td><td>75.92118226600985</td><td>null</td></tr>\n",
              "<tr><td>stddev</td><td>null</td><td>8.4017773522706</td><td>1.712159631548529</td><td>104.92245837948867</td><td>40.52065912106347</td><td>847.0043282393513</td><td>2.8033588163425462</td><td>3.7487373454558743</td><td>null</td></tr>\n",
              "<tr><td>min</td><td>AMC Ambassador Br...</td><td>0</td><td>3</td><td>100.0</td><td>0</td><td>1613.</td><td>10.0</td><td>70</td><td>Europe</td></tr>\n",
              "<tr><td>max</td><td>Volvo Diesel</td><td>9.0</td><td>8</td><td>98.00</td><td>98.00</td><td>5140.</td><td>9.50</td><td>82</td><td>US</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_O4QdO4eHav",
        "outputId": "7712a6f9-58d6-4500-90a6-4b4ce5c6208a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Car',\n",
              " 'MPG',\n",
              " 'Cylinders',\n",
              " 'Displacement',\n",
              " 'Horsepower',\n",
              " 'Weight',\n",
              " 'Acceleration',\n",
              " 'Model',\n",
              " 'Origin']"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"Car\").show(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU9zHuRHePmx",
        "outputId": "a921a8d5-855d-40f1-9699-b19237169f0e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                 Car|\n",
            "+--------------------+\n",
            "|Chevrolet Chevell...|\n",
            "|   Buick Skylark 320|\n",
            "|  Plymouth Satellite|\n",
            "|       AMC Rebel SST|\n",
            "+--------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(['Car','Cylinders']).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKbFGf-FeukJ",
        "outputId": "3ff95159-94be-48eb-f5fb-b5ebdceba2fb"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------+\n",
            "|                 Car|Cylinders|\n",
            "+--------------------+---------+\n",
            "|Chevrolet Chevell...|        8|\n",
            "|   Buick Skylark 320|        8|\n",
            "|  Plymouth Satellite|        8|\n",
            "|       AMC Rebel SST|        8|\n",
            "|         Ford Torino|        8|\n",
            "+--------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('Model','Origin','Cylinders').count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "Dxqq0AsxfMbK",
        "outputId": "cde3ca53-ffc4-4b5b-8395-dd2b8cb6cd41"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-----+------+---------+-----+\n",
              "|Model|Origin|Cylinders|count|\n",
              "+-----+------+---------+-----+\n",
              "|   80|    US|        6|    1|\n",
              "|   73|    US|        8|   20|\n",
              "|   79|    US|        4|    7|\n",
              "|   76|    US|        6|    8|\n",
              "|   72| Japan|        4|    4|\n",
              "|   81|    US|        4|    8|\n",
              "|   72|    US|        8|   13|\n",
              "|   82|    US|        6|    3|\n",
              "|   76|    US|        4|    5|\n",
              "|   72|Europe|        4|    5|\n",
              "|   78|    US|        6|   10|\n",
              "|   72| Japan|        3|    1|\n",
              "|   75|Europe|        4|    6|\n",
              "|   78|    US|        4|    6|\n",
              "|   75| Japan|        4|    4|\n",
              "|   79|Europe|        4|    3|\n",
              "|   80|Europe|        4|    8|\n",
              "|   71|    US|        6|    8|\n",
              "|   76|Europe|        4|    7|\n",
              "|   81|    US|        6|    4|\n",
              "+-----+------+---------+-----+\n",
              "only showing top 20 rows"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>Model</th><th>Origin</th><th>Cylinders</th><th>count</th></tr>\n",
              "<tr><td>80</td><td>US</td><td>6</td><td>1</td></tr>\n",
              "<tr><td>73</td><td>US</td><td>8</td><td>20</td></tr>\n",
              "<tr><td>79</td><td>US</td><td>4</td><td>7</td></tr>\n",
              "<tr><td>76</td><td>US</td><td>6</td><td>8</td></tr>\n",
              "<tr><td>72</td><td>Japan</td><td>4</td><td>4</td></tr>\n",
              "<tr><td>81</td><td>US</td><td>4</td><td>8</td></tr>\n",
              "<tr><td>72</td><td>US</td><td>8</td><td>13</td></tr>\n",
              "<tr><td>82</td><td>US</td><td>6</td><td>3</td></tr>\n",
              "<tr><td>76</td><td>US</td><td>4</td><td>5</td></tr>\n",
              "<tr><td>72</td><td>Europe</td><td>4</td><td>5</td></tr>\n",
              "<tr><td>78</td><td>US</td><td>6</td><td>10</td></tr>\n",
              "<tr><td>72</td><td>Japan</td><td>3</td><td>1</td></tr>\n",
              "<tr><td>75</td><td>Europe</td><td>4</td><td>6</td></tr>\n",
              "<tr><td>78</td><td>US</td><td>4</td><td>6</td></tr>\n",
              "<tr><td>75</td><td>Japan</td><td>4</td><td>4</td></tr>\n",
              "<tr><td>79</td><td>Europe</td><td>4</td><td>3</td></tr>\n",
              "<tr><td>80</td><td>Europe</td><td>4</td><td>8</td></tr>\n",
              "<tr><td>71</td><td>US</td><td>6</td><td>8</td></tr>\n",
              "<tr><td>76</td><td>Europe</td><td>4</td><td>7</td></tr>\n",
              "<tr><td>81</td><td>US</td><td>6</td><td>4</td></tr>\n",
              "</table>\n",
              "only showing top 20 rows\n"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('Cylinders').distinct()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "Gdo20-w5feBD",
        "outputId": "95899cc6-bec9-4920-ad53-6abcb9c0c1c3"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+---------+\n",
              "|Cylinders|\n",
              "+---------+\n",
              "|        3|\n",
              "|        8|\n",
              "|        5|\n",
              "|        6|\n",
              "|        4|\n",
              "+---------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>Cylinders</th></tr>\n",
              "<tr><td>3</td></tr>\n",
              "<tr><td>8</td></tr>\n",
              "<tr><td>5</td></tr>\n",
              "<tr><td>6</td></tr>\n",
              "<tr><td>4</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('Origin').distinct()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "3PzFPOhjgT3C",
        "outputId": "ea80c00d-8d47-4af8-d9e8-b0f8f606b36d"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+------+\n",
              "|Origin|\n",
              "+------+\n",
              "|Europe|\n",
              "|    US|\n",
              "| Japan|\n",
              "+------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>Origin</th></tr>\n",
              "<tr><td>Europe</td></tr>\n",
              "<tr><td>US</td></tr>\n",
              "<tr><td>Japan</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select('Cylinders','Origin').distinct()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "Fq4MsZa4gZjD",
        "outputId": "5945172d-566d-4e0f-f338-30510679cf12"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+---------+------+\n",
              "|Cylinders|Origin|\n",
              "+---------+------+\n",
              "|        6|    US|\n",
              "|        8|    US|\n",
              "|        6|Europe|\n",
              "|        4|    US|\n",
              "|        5|Europe|\n",
              "|        3| Japan|\n",
              "|        4|Europe|\n",
              "|        4| Japan|\n",
              "|        6| Japan|\n",
              "+---------+------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>Cylinders</th><th>Origin</th></tr>\n",
              "<tr><td>6</td><td>US</td></tr>\n",
              "<tr><td>8</td><td>US</td></tr>\n",
              "<tr><td>6</td><td>Europe</td></tr>\n",
              "<tr><td>4</td><td>US</td></tr>\n",
              "<tr><td>5</td><td>Europe</td></tr>\n",
              "<tr><td>3</td><td>Japan</td></tr>\n",
              "<tr><td>4</td><td>Europe</td></tr>\n",
              "<tr><td>4</td><td>Japan</td></tr>\n",
              "<tr><td>6</td><td>Japan</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RDD**\n",
        "---\n",
        "RDDs are the most important component of PySpark. PySpark RDD is one of the fundamental data structures for handling both structured and unstructured data and lacks any schema. Compared to network and disc sharing, PySpark RDD speeds up in-memory data sharing by 10 to 100 times.\n",
        "\n",
        "**What Is PySpark RDD?**\n",
        "\n",
        "---\n",
        "\n",
        "Resilient Distributed Datasets, often known as RDDs, are the components used in a cluster's parallel processing that run and operate across numerous nodes. Since RDDs are immutable elements, you cannot alter them after creation. Because RDDs are fault-tolerant, they will immediately recover from any failure. These RDDs allow you to do various operations to complete a certain goal.\n",
        "\n",
        "**Use of PySpark RDD**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Spark uses the RDD (Resilient Distributed Dataset) data structure as a foundation to conduct MapReduce operations quickly and effectively.\n",
        "\n",
        "Replication, serialization, and disc IO all contribute to the lengthy time required for data exchange in MapReduce. Read-write operations in Hadoop applications use more than 90% of the processing time. Researchers developed this in-memory processing RDD approach as a result. Compared to network and disc, using RDDs accelerated data sharing in memory by a factor of 10 to 100.\n",
        "\n",
        "**Operations With PySpark RDDs**\n",
        "\n",
        "---\n",
        "\n",
        "A fundamental data structure in PySpark is the resilient distributed dataset or RDD. A low-level object, PySpark RDDs are very effective at handling distributed jobs. \n",
        "\n",
        "Any task can be completed using a collection of operations in PySpark RDD. These processes fall into two categories:\n",
        "\n",
        "- Transformations\n",
        "- Actions\n",
        "\n",
        "Transformations are a kind of operation that accepts an RDD as input and outputs another RDD. An RDD that has transformed returns a new RDD; the old RDD remains unchanged and is hence immutable. The Transformation generates a Directed Acyclic Graph, or DAG, for computations after applying it and stops after performing any operations. \n",
        "\n",
        "Actions are a kind of operation used to produce a single value from an RDD. These techniques are used to change a resultant RDD into a non-RDD value, eliminating the inefficiency of the RDD transformation."
      ],
      "metadata": {
        "id": "cRNvVxS1h2Zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformations\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Transformations are the process which are used to create a new RDD. It follows the principle of Lazy Evaluations (the execution will not start until an action is triggered). Few of transformations are given below:\n",
        "\n",
        "- map\n",
        "- flatMap\n",
        "- filter\n",
        "- distinct\n",
        "- reduceByKey\n",
        "- mapPartitions\n",
        "- sortBy\n",
        "\n",
        "Action\n",
        "\n",
        "---\n",
        "\n",
        "Actions are the processes which are applied on an RDD to initiate Apache Spark to apply calculation and pass the result back to driver. Few actions are following:\n",
        "\n",
        "- collect\n",
        "- collectAsMap\n",
        "- reduce\n",
        "- countByKey/countByValue\n",
        "- take\n",
        "- first"
      ],
      "metadata": {
        "id": "vZOnEdYBjepJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **sample RDD: word count problem**"
      ],
      "metadata": {
        "id": "k2DtFxNQkwWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input file: \n",
        "\n",
        "https://raw.githubusercontent.com/nivdul/spark-in-practice-scala/master/data/wordcount.txt\n"
      ],
      "metadata": {
        "id": "7_BsH7-JmIgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/nivdul/spark-in-practice-scala/master/data/wordcount.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJo3i-OOnGQJ",
        "outputId": "08fb001a-06b0-4169-9cb2-1ecdc46b96ee"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-02 18:38:11--  https://raw.githubusercontent.com/nivdul/spark-in-practice-scala/master/data/wordcount.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4921 (4.8K) [text/plain]\n",
            "Saving to: ‘wordcount.txt’\n",
            "\n",
            "\rwordcount.txt         0%[                    ]       0  --.-KB/s               \rwordcount.txt       100%[===================>]   4.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-02 18:38:11 (42.5 MB/s) - ‘wordcount.txt’ saved [4921/4921]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "9Vkvm3gAlcAf"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = sc.textFile(\"wordcount.txt\")"
      ],
      "metadata": {
        "id": "TqNfXWxvmhTl"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# words.collect()"
      ],
      "metadata": {
        "id": "zX8WhBDOmurV"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize"
      ],
      "metadata": {
        "id": "YuS5r7VToaxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = words.flatMap(lambda line: line.split(\" \")) # tokenize\n",
        "print(words.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cmzwAy-mwkX",
        "outputId": "572180b5-eeea-4241-ffd7-4c572189ba7f"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['word', 'count', 'from', 'Wikipedia', 'the', 'free', 'encyclopedia', 'the', 'word', 'count', 'is', 'the', 'number', 'of', 'words', 'in', 'a', 'document', 'or', 'passage', 'of', 'text', 'Word', 'counting', 'may', 'be', 'needed', 'when', 'a', 'text', 'is', 'required', 'to', 'stay', 'within', 'certain', 'numbers', 'of', 'words', 'This', 'may', 'particularly', 'be', 'the', 'case', 'in', 'academia', 'legal', 'proceedings', 'journalism', 'and', 'advertising', 'Word', 'count', 'is', 'commonly', 'used', 'by', 'translators', 'to', 'determine', 'the', 'price', 'for', 'the', 'translation', 'job', 'Word', 'counts', 'may', 'also', 'be', 'used', 'to', 'calculate', 'measures', 'of', 'readability', 'and', 'to', 'measure', 'typing', 'and', 'reading', 'speeds', 'usually', 'in', 'words', 'per', 'minute', 'When', 'converting', 'character', 'counts', 'to', 'words', 'a', 'measure', 'of', 'five', 'or', 'six', 'characters', 'to', 'a', 'word', 'is', 'generally', 'used', 'Contents', 'Details', 'and', 'variations', 'of', 'definition', 'Software', 'In', 'fiction', 'In', 'non', 'fiction', 'See', 'also', 'References', 'Sources', 'External', 'links', 'Details', 'and', 'variations', 'of', 'definition', 'This', 'section', 'does', 'not', 'cite', 'any', 'references', 'or', 'sources', 'Please', 'help', 'improve', 'this', 'section', 'by', 'adding', 'citations', 'to', 'reliable', 'sources', 'Unsourced', 'material', 'may', 'be', 'challenged', 'and', 'removed', 'Variations', 'in', 'the', 'operational', 'definitions', 'of', 'how', 'to', 'count', 'the', 'words', 'can', 'occur', 'namely', 'what', 'counts', 'as', 'a', 'word', 'and', 'which', 'words', \"don't\", 'count', 'toward', 'the', 'total', 'However', 'especially', 'since', 'the', 'advent', 'of', 'widespread', 'word', 'processing', 'there', 'is', 'a', 'broad', 'consensus', 'on', 'these', 'operational', 'definitions', 'and', 'hence', 'the', 'bottom', 'line', 'integer', 'result', 'The', 'consensus', 'is', 'to', 'accept', 'the', 'text', 'segmentation', 'rules', 'generally', 'found', 'in', 'most', 'word', 'processing', 'software', 'including', 'how', 'word', 'boundaries', 'are', 'determined', 'which', 'depends', 'on', 'how', 'word', 'dividers', 'are', 'defined', 'The', 'first', 'trait', 'of', 'that', 'definition', 'is', 'that', 'a', 'space', 'any', 'of', 'various', 'whitespace', 'characters', 'such', 'as', 'a', 'regular', 'word', 'space', 'an', 'em', 'space', 'or', 'a', 'tab', 'character', 'is', 'a', 'word', 'divider', 'Usually', 'a', 'hyphen', 'or', 'a', 'slash', 'is', 'too', 'Different', 'word', 'counting', 'programs', 'may', 'give', 'varying', 'results', 'depending', 'on', 'the', 'text', 'segmentation', 'rule', 'details', 'and', 'on', 'whether', 'words', 'outside', 'the', 'main', 'text', 'such', 'as', 'footnotes', 'endnotes', 'or', 'hidden', 'text)', 'are', 'counted', 'But', 'the', 'behavior', 'of', 'most', 'major', 'word', 'processing', 'applications', 'is', 'broadly', 'similar', 'However', 'during', 'the', 'era', 'when', 'school', 'assignments', 'were', 'done', 'in', 'handwriting', 'or', 'with', 'typewriters', 'the', 'rules', 'for', 'these', 'definitions', 'often', 'differed', 'from', 'todays', 'consensus', 'Most', 'importantly', 'many', 'students', 'were', 'drilled', 'on', 'the', 'rule', 'that', 'certain', 'words', \"don't\", 'count', 'usually', 'articles', 'namely', 'a', 'an', 'the', 'but', 'sometimes', 'also', 'others', 'such', 'as', 'conjunctions', 'for', 'example', 'and', 'or', 'but', 'and', 'some', 'prepositions', 'usually', 'to', 'of', 'Hyphenated', 'permanent', 'compounds', 'such', 'as', 'follow', 'up', 'noun', 'or', 'long', 'term', 'adjective', 'were', 'counted', 'as', 'one', 'word', 'To', 'save', 'the', 'time', 'and', 'effort', 'of', 'counting', 'word', 'by', 'word', 'often', 'a', 'rule', 'of', 'thumb', 'for', 'the', 'average', 'number', 'of', 'words', 'per', 'line', 'was', 'used', 'such', 'as', '10', 'words', 'per', 'line', 'These', 'rules', 'have', 'fallen', 'by', 'the', 'wayside', 'in', 'the', 'word', 'processing', 'era', 'the', 'word', 'count', 'feature', 'of', 'such', 'software', 'which', 'follows', 'the', 'text', 'segmentation', 'rules', 'mentioned', 'earlier', 'is', 'now', 'the', 'standard', 'arbiter', 'because', 'it', 'is', 'largely', 'consistent', 'across', 'documents', 'and', 'applications', 'and', 'because', 'it', 'is', 'fast', 'effortless', 'and', 'costless', 'already', 'included', 'with', 'the', 'application', 'As', 'for', 'which', 'sections', 'of', 'a', 'document', 'count', 'toward', 'the', 'total', 'such', 'as', 'footnotes', 'endnotes', 'abstracts', 'reference', 'lists', 'and', 'bibliographies', 'tables', 'figure', 'captions', 'hidden', 'text', 'the', 'person', 'in', 'charge', 'teacher', 'client', 'can', 'define', 'their', 'choice', 'and', 'users', 'students', 'workers', 'can', 'simply', 'select', 'or', 'exclude', 'the', 'elements', 'accordingly', 'and', 'watch', 'the', 'word', 'count', 'automatically', 'update', 'Software', 'Modern', 'web', 'browsers', 'support', 'word', 'counting', 'via', 'extensions', 'via', 'a', 'JavaScript', 'bookmarklet', 'or', 'a', 'script', 'that', 'is', 'hosted', 'in', 'a', 'website', 'Most', 'word', 'processors', 'can', 'also', 'count', 'words', 'Unix', 'like', 'systems', 'include', 'a', 'program', 'wc', 'specifically', 'for', 'word', 'counting', 'As', 'explained', 'earlier', 'different', 'word', 'counting', 'programs', 'may', 'give', 'varying', 'results', 'depending', 'on', 'the', 'text', 'segmentation', 'rule', 'details', 'The', 'exact', 'number', 'of', 'words', 'often', 'is', 'not', 'a', 'strict', 'requirement', 'thus', 'the', 'variation', 'is', 'acceptable', 'In', 'fiction', 'Novelist', 'Jane', 'Smiley', 'suggests', 'that', 'length', 'is', 'an', 'important', 'quality', 'of', 'the', 'novel', 'However', 'novels', 'can', 'vary', 'tremendously', 'in', 'length', 'Smiley', 'lists', 'novels', 'as', 'typically', 'being', 'between', 'and', 'words', 'while', 'National', 'Novel', 'Writing', 'Month', 'requires', 'its', 'novels', 'to', 'be', 'at', 'least', 'words', 'There', 'are', 'no', 'firm', 'rules', 'for', 'example', 'the', 'boundary', 'between', 'a', 'novella', 'and', 'a', 'novel', 'is', 'arbitrary', 'and', 'a', 'literary', 'work', 'may', 'be', 'difficult', 'to', 'categorise', 'But', 'while', 'the', 'length', 'of', 'a', 'novel', 'is', 'to', 'a', 'large', 'extent', 'up', 'to', 'its', 'writer', 'lengths', 'may', 'also', 'vary', 'by', 'subgenre', 'many', 'chapter', 'books', 'for', 'children', 'start', 'at', 'a', 'length', 'of', 'about', 'words', 'and', 'a', 'typical', 'mystery', 'novel', 'might', 'be', 'in', 'the', 'to', 'word', 'range', 'while', 'a', 'thriller', 'could', 'be', 'over', 'words', 'The', 'Science', 'Fiction', 'and', 'Fantasy', 'Writers', 'of', 'America', 'specifies', 'word', 'lengths', 'for', 'each', 'category', 'of', 'its', 'Nebula', 'award', 'categories', 'Classification\\tWord', 'count', 'Novel', 'over', 'words', 'Novella', 'to', 'words', 'Novelette', 'to', 'words', 'Short', 'story', 'under', 'words', 'In', 'non', 'fiction', 'The', 'acceptable', 'length', 'of', 'an', 'academic', 'dissertation', 'varies', 'greatly', 'dependent', 'predominantly', 'on', 'the', 'subject', 'Numerous', 'American', 'universities', 'limit', 'Ph.D.', 'dissertations', 'to', 'at', 'most', 'words', 'barring', 'special', 'permission', 'for', 'exceeding', 'this', 'limit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "map operation"
      ],
      "metadata": {
        "id": "fPJaBEajoc9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_map = words.map(lambda word:(word,1))\n",
        "print(word_map.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VQ4sJ3Qn6iU",
        "outputId": "d057522c-7793-4c03-b141-552a0480cbdd"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('word', 1), ('count', 1), ('from', 1), ('Wikipedia', 1), ('the', 1), ('free', 1), ('encyclopedia', 1), ('the', 1), ('word', 1), ('count', 1), ('is', 1), ('the', 1), ('number', 1), ('of', 1), ('words', 1), ('in', 1), ('a', 1), ('document', 1), ('or', 1), ('passage', 1), ('of', 1), ('text', 1), ('Word', 1), ('counting', 1), ('may', 1), ('be', 1), ('needed', 1), ('when', 1), ('a', 1), ('text', 1), ('is', 1), ('required', 1), ('to', 1), ('stay', 1), ('within', 1), ('certain', 1), ('numbers', 1), ('of', 1), ('words', 1), ('This', 1), ('may', 1), ('particularly', 1), ('be', 1), ('the', 1), ('case', 1), ('in', 1), ('academia', 1), ('legal', 1), ('proceedings', 1), ('journalism', 1), ('and', 1), ('advertising', 1), ('Word', 1), ('count', 1), ('is', 1), ('commonly', 1), ('used', 1), ('by', 1), ('translators', 1), ('to', 1), ('determine', 1), ('the', 1), ('price', 1), ('for', 1), ('the', 1), ('translation', 1), ('job', 1), ('Word', 1), ('counts', 1), ('may', 1), ('also', 1), ('be', 1), ('used', 1), ('to', 1), ('calculate', 1), ('measures', 1), ('of', 1), ('readability', 1), ('and', 1), ('to', 1), ('measure', 1), ('typing', 1), ('and', 1), ('reading', 1), ('speeds', 1), ('usually', 1), ('in', 1), ('words', 1), ('per', 1), ('minute', 1), ('When', 1), ('converting', 1), ('character', 1), ('counts', 1), ('to', 1), ('words', 1), ('a', 1), ('measure', 1), ('of', 1), ('five', 1), ('or', 1), ('six', 1), ('characters', 1), ('to', 1), ('a', 1), ('word', 1), ('is', 1), ('generally', 1), ('used', 1), ('Contents', 1), ('Details', 1), ('and', 1), ('variations', 1), ('of', 1), ('definition', 1), ('Software', 1), ('In', 1), ('fiction', 1), ('In', 1), ('non', 1), ('fiction', 1), ('See', 1), ('also', 1), ('References', 1), ('Sources', 1), ('External', 1), ('links', 1), ('Details', 1), ('and', 1), ('variations', 1), ('of', 1), ('definition', 1), ('This', 1), ('section', 1), ('does', 1), ('not', 1), ('cite', 1), ('any', 1), ('references', 1), ('or', 1), ('sources', 1), ('Please', 1), ('help', 1), ('improve', 1), ('this', 1), ('section', 1), ('by', 1), ('adding', 1), ('citations', 1), ('to', 1), ('reliable', 1), ('sources', 1), ('Unsourced', 1), ('material', 1), ('may', 1), ('be', 1), ('challenged', 1), ('and', 1), ('removed', 1), ('Variations', 1), ('in', 1), ('the', 1), ('operational', 1), ('definitions', 1), ('of', 1), ('how', 1), ('to', 1), ('count', 1), ('the', 1), ('words', 1), ('can', 1), ('occur', 1), ('namely', 1), ('what', 1), ('counts', 1), ('as', 1), ('a', 1), ('word', 1), ('and', 1), ('which', 1), ('words', 1), (\"don't\", 1), ('count', 1), ('toward', 1), ('the', 1), ('total', 1), ('However', 1), ('especially', 1), ('since', 1), ('the', 1), ('advent', 1), ('of', 1), ('widespread', 1), ('word', 1), ('processing', 1), ('there', 1), ('is', 1), ('a', 1), ('broad', 1), ('consensus', 1), ('on', 1), ('these', 1), ('operational', 1), ('definitions', 1), ('and', 1), ('hence', 1), ('the', 1), ('bottom', 1), ('line', 1), ('integer', 1), ('result', 1), ('The', 1), ('consensus', 1), ('is', 1), ('to', 1), ('accept', 1), ('the', 1), ('text', 1), ('segmentation', 1), ('rules', 1), ('generally', 1), ('found', 1), ('in', 1), ('most', 1), ('word', 1), ('processing', 1), ('software', 1), ('including', 1), ('how', 1), ('word', 1), ('boundaries', 1), ('are', 1), ('determined', 1), ('which', 1), ('depends', 1), ('on', 1), ('how', 1), ('word', 1), ('dividers', 1), ('are', 1), ('defined', 1), ('The', 1), ('first', 1), ('trait', 1), ('of', 1), ('that', 1), ('definition', 1), ('is', 1), ('that', 1), ('a', 1), ('space', 1), ('any', 1), ('of', 1), ('various', 1), ('whitespace', 1), ('characters', 1), ('such', 1), ('as', 1), ('a', 1), ('regular', 1), ('word', 1), ('space', 1), ('an', 1), ('em', 1), ('space', 1), ('or', 1), ('a', 1), ('tab', 1), ('character', 1), ('is', 1), ('a', 1), ('word', 1), ('divider', 1), ('Usually', 1), ('a', 1), ('hyphen', 1), ('or', 1), ('a', 1), ('slash', 1), ('is', 1), ('too', 1), ('Different', 1), ('word', 1), ('counting', 1), ('programs', 1), ('may', 1), ('give', 1), ('varying', 1), ('results', 1), ('depending', 1), ('on', 1), ('the', 1), ('text', 1), ('segmentation', 1), ('rule', 1), ('details', 1), ('and', 1), ('on', 1), ('whether', 1), ('words', 1), ('outside', 1), ('the', 1), ('main', 1), ('text', 1), ('such', 1), ('as', 1), ('footnotes', 1), ('endnotes', 1), ('or', 1), ('hidden', 1), ('text)', 1), ('are', 1), ('counted', 1), ('But', 1), ('the', 1), ('behavior', 1), ('of', 1), ('most', 1), ('major', 1), ('word', 1), ('processing', 1), ('applications', 1), ('is', 1), ('broadly', 1), ('similar', 1), ('However', 1), ('during', 1), ('the', 1), ('era', 1), ('when', 1), ('school', 1), ('assignments', 1), ('were', 1), ('done', 1), ('in', 1), ('handwriting', 1), ('or', 1), ('with', 1), ('typewriters', 1), ('the', 1), ('rules', 1), ('for', 1), ('these', 1), ('definitions', 1), ('often', 1), ('differed', 1), ('from', 1), ('todays', 1), ('consensus', 1), ('Most', 1), ('importantly', 1), ('many', 1), ('students', 1), ('were', 1), ('drilled', 1), ('on', 1), ('the', 1), ('rule', 1), ('that', 1), ('certain', 1), ('words', 1), (\"don't\", 1), ('count', 1), ('usually', 1), ('articles', 1), ('namely', 1), ('a', 1), ('an', 1), ('the', 1), ('but', 1), ('sometimes', 1), ('also', 1), ('others', 1), ('such', 1), ('as', 1), ('conjunctions', 1), ('for', 1), ('example', 1), ('and', 1), ('or', 1), ('but', 1), ('and', 1), ('some', 1), ('prepositions', 1), ('usually', 1), ('to', 1), ('of', 1), ('Hyphenated', 1), ('permanent', 1), ('compounds', 1), ('such', 1), ('as', 1), ('follow', 1), ('up', 1), ('noun', 1), ('or', 1), ('long', 1), ('term', 1), ('adjective', 1), ('were', 1), ('counted', 1), ('as', 1), ('one', 1), ('word', 1), ('To', 1), ('save', 1), ('the', 1), ('time', 1), ('and', 1), ('effort', 1), ('of', 1), ('counting', 1), ('word', 1), ('by', 1), ('word', 1), ('often', 1), ('a', 1), ('rule', 1), ('of', 1), ('thumb', 1), ('for', 1), ('the', 1), ('average', 1), ('number', 1), ('of', 1), ('words', 1), ('per', 1), ('line', 1), ('was', 1), ('used', 1), ('such', 1), ('as', 1), ('10', 1), ('words', 1), ('per', 1), ('line', 1), ('These', 1), ('rules', 1), ('have', 1), ('fallen', 1), ('by', 1), ('the', 1), ('wayside', 1), ('in', 1), ('the', 1), ('word', 1), ('processing', 1), ('era', 1), ('the', 1), ('word', 1), ('count', 1), ('feature', 1), ('of', 1), ('such', 1), ('software', 1), ('which', 1), ('follows', 1), ('the', 1), ('text', 1), ('segmentation', 1), ('rules', 1), ('mentioned', 1), ('earlier', 1), ('is', 1), ('now', 1), ('the', 1), ('standard', 1), ('arbiter', 1), ('because', 1), ('it', 1), ('is', 1), ('largely', 1), ('consistent', 1), ('across', 1), ('documents', 1), ('and', 1), ('applications', 1), ('and', 1), ('because', 1), ('it', 1), ('is', 1), ('fast', 1), ('effortless', 1), ('and', 1), ('costless', 1), ('already', 1), ('included', 1), ('with', 1), ('the', 1), ('application', 1), ('As', 1), ('for', 1), ('which', 1), ('sections', 1), ('of', 1), ('a', 1), ('document', 1), ('count', 1), ('toward', 1), ('the', 1), ('total', 1), ('such', 1), ('as', 1), ('footnotes', 1), ('endnotes', 1), ('abstracts', 1), ('reference', 1), ('lists', 1), ('and', 1), ('bibliographies', 1), ('tables', 1), ('figure', 1), ('captions', 1), ('hidden', 1), ('text', 1), ('the', 1), ('person', 1), ('in', 1), ('charge', 1), ('teacher', 1), ('client', 1), ('can', 1), ('define', 1), ('their', 1), ('choice', 1), ('and', 1), ('users', 1), ('students', 1), ('workers', 1), ('can', 1), ('simply', 1), ('select', 1), ('or', 1), ('exclude', 1), ('the', 1), ('elements', 1), ('accordingly', 1), ('and', 1), ('watch', 1), ('the', 1), ('word', 1), ('count', 1), ('automatically', 1), ('update', 1), ('Software', 1), ('Modern', 1), ('web', 1), ('browsers', 1), ('support', 1), ('word', 1), ('counting', 1), ('via', 1), ('extensions', 1), ('via', 1), ('a', 1), ('JavaScript', 1), ('bookmarklet', 1), ('or', 1), ('a', 1), ('script', 1), ('that', 1), ('is', 1), ('hosted', 1), ('in', 1), ('a', 1), ('website', 1), ('Most', 1), ('word', 1), ('processors', 1), ('can', 1), ('also', 1), ('count', 1), ('words', 1), ('Unix', 1), ('like', 1), ('systems', 1), ('include', 1), ('a', 1), ('program', 1), ('wc', 1), ('specifically', 1), ('for', 1), ('word', 1), ('counting', 1), ('As', 1), ('explained', 1), ('earlier', 1), ('different', 1), ('word', 1), ('counting', 1), ('programs', 1), ('may', 1), ('give', 1), ('varying', 1), ('results', 1), ('depending', 1), ('on', 1), ('the', 1), ('text', 1), ('segmentation', 1), ('rule', 1), ('details', 1), ('The', 1), ('exact', 1), ('number', 1), ('of', 1), ('words', 1), ('often', 1), ('is', 1), ('not', 1), ('a', 1), ('strict', 1), ('requirement', 1), ('thus', 1), ('the', 1), ('variation', 1), ('is', 1), ('acceptable', 1), ('In', 1), ('fiction', 1), ('Novelist', 1), ('Jane', 1), ('Smiley', 1), ('suggests', 1), ('that', 1), ('length', 1), ('is', 1), ('an', 1), ('important', 1), ('quality', 1), ('of', 1), ('the', 1), ('novel', 1), ('However', 1), ('novels', 1), ('can', 1), ('vary', 1), ('tremendously', 1), ('in', 1), ('length', 1), ('Smiley', 1), ('lists', 1), ('novels', 1), ('as', 1), ('typically', 1), ('being', 1), ('between', 1), ('and', 1), ('words', 1), ('while', 1), ('National', 1), ('Novel', 1), ('Writing', 1), ('Month', 1), ('requires', 1), ('its', 1), ('novels', 1), ('to', 1), ('be', 1), ('at', 1), ('least', 1), ('words', 1), ('There', 1), ('are', 1), ('no', 1), ('firm', 1), ('rules', 1), ('for', 1), ('example', 1), ('the', 1), ('boundary', 1), ('between', 1), ('a', 1), ('novella', 1), ('and', 1), ('a', 1), ('novel', 1), ('is', 1), ('arbitrary', 1), ('and', 1), ('a', 1), ('literary', 1), ('work', 1), ('may', 1), ('be', 1), ('difficult', 1), ('to', 1), ('categorise', 1), ('But', 1), ('while', 1), ('the', 1), ('length', 1), ('of', 1), ('a', 1), ('novel', 1), ('is', 1), ('to', 1), ('a', 1), ('large', 1), ('extent', 1), ('up', 1), ('to', 1), ('its', 1), ('writer', 1), ('lengths', 1), ('may', 1), ('also', 1), ('vary', 1), ('by', 1), ('subgenre', 1), ('many', 1), ('chapter', 1), ('books', 1), ('for', 1), ('children', 1), ('start', 1), ('at', 1), ('a', 1), ('length', 1), ('of', 1), ('about', 1), ('words', 1), ('and', 1), ('a', 1), ('typical', 1), ('mystery', 1), ('novel', 1), ('might', 1), ('be', 1), ('in', 1), ('the', 1), ('to', 1), ('word', 1), ('range', 1), ('while', 1), ('a', 1), ('thriller', 1), ('could', 1), ('be', 1), ('over', 1), ('words', 1), ('The', 1), ('Science', 1), ('Fiction', 1), ('and', 1), ('Fantasy', 1), ('Writers', 1), ('of', 1), ('America', 1), ('specifies', 1), ('word', 1), ('lengths', 1), ('for', 1), ('each', 1), ('category', 1), ('of', 1), ('its', 1), ('Nebula', 1), ('award', 1), ('categories', 1), ('Classification\\tWord', 1), ('count', 1), ('Novel', 1), ('over', 1), ('words', 1), ('Novella', 1), ('to', 1), ('words', 1), ('Novelette', 1), ('to', 1), ('words', 1), ('Short', 1), ('story', 1), ('under', 1), ('words', 1), ('In', 1), ('non', 1), ('fiction', 1), ('The', 1), ('acceptable', 1), ('length', 1), ('of', 1), ('an', 1), ('academic', 1), ('dissertation', 1), ('varies', 1), ('greatly', 1), ('dependent', 1), ('predominantly', 1), ('on', 1), ('the', 1), ('subject', 1), ('Numerous', 1), ('American', 1), ('universities', 1), ('limit', 1), ('Ph.D.', 1), ('dissertations', 1), ('to', 1), ('at', 1), ('most', 1), ('words', 1), ('barring', 1), ('special', 1), ('permission', 1), ('for', 1), ('exceeding', 1), ('this', 1), ('limit', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reduce operation"
      ],
      "metadata": {
        "id": "QeXihKn2ofHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_count = word_map.reduceByKey(lambda a,b:a+b)\n",
        "print(word_count.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baQ032choWsm",
        "outputId": "37c88a33-5fbc-496e-826f-f7dc9f537072"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('word', 24), ('count', 11), ('from', 2), ('Wikipedia', 1), ('the', 38), ('free', 1), ('encyclopedia', 1), ('is', 19), ('number', 3), ('of', 25), ('words', 21), ('in', 11), ('a', 28), ('document', 2), ('or', 11), ('passage', 1), ('text', 8), ('Word', 3), ('counting', 6), ('may', 8), ('be', 8), ('needed', 1), ('when', 2), ('required', 1), ('to', 18), ('stay', 1), ('within', 1), ('certain', 2), ('numbers', 1), ('This', 2), ('particularly', 1), ('case', 1), ('academia', 1), ('legal', 1), ('proceedings', 1), ('journalism', 1), ('and', 23), ('advertising', 1), ('commonly', 1), ('used', 4), ('by', 5), ('translators', 1), ('determine', 1), ('price', 1), ('for', 10), ('translation', 1), ('job', 1), ('counts', 3), ('also', 5), ('calculate', 1), ('measures', 1), ('readability', 1), ('measure', 2), ('typing', 1), ('reading', 1), ('speeds', 1), ('usually', 3), ('per', 3), ('minute', 1), ('When', 1), ('converting', 1), ('character', 2), ('five', 1), ('six', 1), ('characters', 2), ('generally', 2), ('Contents', 1), ('Details', 2), ('variations', 2), ('definition', 3), ('Software', 2), ('In', 4), ('fiction', 4), ('non', 2), ('See', 1), ('References', 1), ('Sources', 1), ('External', 1), ('links', 1), ('section', 2), ('does', 1), ('not', 2), ('cite', 1), ('any', 2), ('references', 1), ('sources', 2), ('Please', 1), ('help', 1), ('improve', 1), ('this', 2), ('adding', 1), ('citations', 1), ('reliable', 1), ('Unsourced', 1), ('material', 1), ('challenged', 1), ('removed', 1), ('Variations', 1), ('operational', 2), ('definitions', 3), ('how', 3), ('can', 5), ('occur', 1), ('namely', 2), ('what', 1), ('as', 9), ('which', 4), (\"don't\", 2), ('toward', 2), ('total', 2), ('However', 3), ('especially', 1), ('since', 1), ('advent', 1), ('widespread', 1), ('processing', 4), ('there', 1), ('broad', 1), ('consensus', 3), ('on', 7), ('these', 2), ('hence', 1), ('bottom', 1), ('line', 3), ('integer', 1), ('result', 1), ('The', 5), ('accept', 1), ('segmentation', 4), ('rules', 5), ('found', 1), ('most', 3), ('software', 2), ('including', 1), ('boundaries', 1), ('are', 4), ('determined', 1), ('depends', 1), ('dividers', 1), ('defined', 1), ('first', 1), ('trait', 1), ('that', 5), ('space', 3), ('various', 1), ('whitespace', 1), ('such', 7), ('regular', 1), ('an', 4), ('em', 1), ('tab', 1), ('divider', 1), ('Usually', 1), ('hyphen', 1), ('slash', 1), ('too', 1), ('Different', 1), ('programs', 2), ('give', 2), ('varying', 2), ('results', 2), ('depending', 2), ('rule', 4), ('details', 2), ('whether', 1), ('outside', 1), ('main', 1), ('footnotes', 2), ('endnotes', 2), ('hidden', 2), ('text)', 1), ('counted', 2), ('But', 2), ('behavior', 1), ('major', 1), ('applications', 2), ('broadly', 1), ('similar', 1), ('during', 1), ('era', 2), ('school', 1), ('assignments', 1), ('were', 3), ('done', 1), ('handwriting', 1), ('with', 2), ('typewriters', 1), ('often', 3), ('differed', 1), ('todays', 1), ('Most', 2), ('importantly', 1), ('many', 2), ('students', 2), ('drilled', 1), ('articles', 1), ('but', 2), ('sometimes', 1), ('others', 1), ('conjunctions', 1), ('example', 2), ('some', 1), ('prepositions', 1), ('Hyphenated', 1), ('permanent', 1), ('compounds', 1), ('follow', 1), ('up', 2), ('noun', 1), ('long', 1), ('term', 1), ('adjective', 1), ('one', 1), ('To', 1), ('save', 1), ('time', 1), ('effort', 1), ('thumb', 1), ('average', 1), ('was', 1), ('10', 1), ('These', 1), ('have', 1), ('fallen', 1), ('wayside', 1), ('feature', 1), ('follows', 1), ('mentioned', 1), ('earlier', 2), ('now', 1), ('standard', 1), ('arbiter', 1), ('because', 2), ('it', 2), ('largely', 1), ('consistent', 1), ('across', 1), ('documents', 1), ('fast', 1), ('effortless', 1), ('costless', 1), ('already', 1), ('included', 1), ('application', 1), ('As', 2), ('sections', 1), ('abstracts', 1), ('reference', 1), ('lists', 2), ('bibliographies', 1), ('tables', 1), ('figure', 1), ('captions', 1), ('person', 1), ('charge', 1), ('teacher', 1), ('client', 1), ('define', 1), ('their', 1), ('choice', 1), ('users', 1), ('workers', 1), ('simply', 1), ('select', 1), ('exclude', 1), ('elements', 1), ('accordingly', 1), ('watch', 1), ('automatically', 1), ('update', 1), ('Modern', 1), ('web', 1), ('browsers', 1), ('support', 1), ('via', 2), ('extensions', 1), ('JavaScript', 1), ('bookmarklet', 1), ('script', 1), ('hosted', 1), ('website', 1), ('processors', 1), ('Unix', 1), ('like', 1), ('systems', 1), ('include', 1), ('program', 1), ('wc', 1), ('specifically', 1), ('explained', 1), ('different', 1), ('exact', 1), ('strict', 1), ('requirement', 1), ('thus', 1), ('variation', 1), ('acceptable', 2), ('Novelist', 1), ('Jane', 1), ('Smiley', 2), ('suggests', 1), ('length', 5), ('important', 1), ('quality', 1), ('novel', 4), ('novels', 3), ('vary', 2), ('tremendously', 1), ('typically', 1), ('being', 1), ('between', 2), ('while', 3), ('National', 1), ('Novel', 2), ('Writing', 1), ('Month', 1), ('requires', 1), ('its', 3), ('at', 3), ('least', 1), ('There', 1), ('no', 1), ('firm', 1), ('boundary', 1), ('novella', 1), ('arbitrary', 1), ('literary', 1), ('work', 1), ('difficult', 1), ('categorise', 1), ('large', 1), ('extent', 1), ('writer', 1), ('lengths', 2), ('subgenre', 1), ('chapter', 1), ('books', 1), ('children', 1), ('start', 1), ('about', 1), ('typical', 1), ('mystery', 1), ('might', 1), ('range', 1), ('thriller', 1), ('could', 1), ('over', 2), ('Science', 1), ('Fiction', 1), ('Fantasy', 1), ('Writers', 1), ('America', 1), ('specifies', 1), ('each', 1), ('category', 1), ('Nebula', 1), ('award', 1), ('categories', 1), ('Classification\\tWord', 1), ('Novella', 1), ('Novelette', 1), ('Short', 1), ('story', 1), ('under', 1), ('academic', 1), ('dissertation', 1), ('varies', 1), ('greatly', 1), ('dependent', 1), ('predominantly', 1), ('subject', 1), ('Numerous', 1), ('American', 1), ('universities', 1), ('limit', 2), ('Ph.D.', 1), ('dissertations', 1), ('barring', 1), ('special', 1), ('permission', 1), ('exceeding', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RDD for cars.csv**"
      ],
      "metadata": {
        "id": "NkhNheHwpJDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cars = sc.textFile('cars.csv')\n",
        "print(type(cars))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TG3EGtGouPI",
        "outputId": "a5e1f739-3537-4267-9b01-db9fde25ceee"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run python script as spark-submit job"
      ],
      "metadata": {
        "id": "VvXWCl9N0Evq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir output"
      ],
      "metadata": {
        "id": "FMdYTI3X0JV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! spark-3.1.1-bin-hadoop3.2/bin/spark-submit word_count.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXSqx2u0tcXP",
        "outputId": "ddce5013-7ba7-4b6a-f687-1a9ed931b71b"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/03/02 19:36:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "23/03/02 19:36:55 INFO SparkContext: Running Spark version 3.1.1\n",
            "23/03/02 19:36:55 INFO ResourceUtils: ==============================================================\n",
            "23/03/02 19:36:55 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "23/03/02 19:36:55 INFO ResourceUtils: ==============================================================\n",
            "23/03/02 19:36:55 INFO SparkContext: Submitted application: Colab\n",
            "23/03/02 19:36:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "23/03/02 19:36:55 INFO ResourceProfile: Limiting resource is cpu\n",
            "23/03/02 19:36:55 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "23/03/02 19:36:55 INFO SecurityManager: Changing view acls to: root\n",
            "23/03/02 19:36:55 INFO SecurityManager: Changing modify acls to: root\n",
            "23/03/02 19:36:55 INFO SecurityManager: Changing view acls groups to: \n",
            "23/03/02 19:36:55 INFO SecurityManager: Changing modify acls groups to: \n",
            "23/03/02 19:36:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
            "23/03/02 19:36:55 INFO Utils: Successfully started service 'sparkDriver' on port 41801.\n",
            "23/03/02 19:36:55 INFO SparkEnv: Registering MapOutputTracker\n",
            "23/03/02 19:36:55 INFO SparkEnv: Registering BlockManagerMaster\n",
            "23/03/02 19:36:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "23/03/02 19:36:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "23/03/02 19:36:55 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/03/02 19:36:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4d9b7c4f-e0b4-4f7b-b552-199dc53a1805\n",
            "23/03/02 19:36:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "23/03/02 19:36:56 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "23/03/02 19:36:56 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
            "23/03/02 19:36:56 INFO Utils: Successfully started service 'SparkUI' on port 4051.\n",
            "23/03/02 19:36:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://b513d4386989:4051\n",
            "23/03/02 19:36:56 INFO Executor: Starting executor ID driver on host b513d4386989\n",
            "23/03/02 19:36:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38847.\n",
            "23/03/02 19:36:56 INFO NettyBlockTransferService: Server created on b513d4386989:38847\n",
            "23/03/02 19:36:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "23/03/02 19:36:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, b513d4386989, 38847, None)\n",
            "23/03/02 19:36:56 INFO BlockManagerMasterEndpoint: Registering block manager b513d4386989:38847 with 366.3 MiB RAM, BlockManagerId(driver, b513d4386989, 38847, None)\n",
            "23/03/02 19:36:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, b513d4386989, 38847, None)\n",
            "23/03/02 19:36:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, b513d4386989, 38847, None)\n",
            "23/03/02 19:36:57 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/content/spark-warehouse').\n",
            "23/03/02 19:36:57 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "23/03/02 19:36:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 294.1 KiB, free 366.0 MiB)\n",
            "23/03/02 19:36:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.0 KiB, free 366.0 MiB)\n",
            "23/03/02 19:36:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on b513d4386989:38847 (size: 27.0 KiB, free: 366.3 MiB)\n",
            "23/03/02 19:36:59 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
            "23/03/02 19:36:59 INFO FileInputFormat: Total input files to process : 1\n",
            "23/03/02 19:36:59 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
            "23/03/02 19:36:59 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/03/02 19:36:59 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/03/02 19:36:59 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/03/02 19:37:00 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
            "23/03/02 19:37:00 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /content/word_count.py:14) as input to shuffle 0\n",
            "23/03/02 19:37:00 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:83) with 1 output partitions\n",
            "23/03/02 19:37:00 INFO DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:83)\n",
            "23/03/02 19:37:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
            "23/03/02 19:37:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
            "23/03/02 19:37:00 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/word_count.py:14), which has no missing parents\n",
            "23/03/02 19:37:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 11.9 KiB, free 366.0 MiB)\n",
            "23/03/02 19:37:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 366.0 MiB)\n",
            "23/03/02 19:37:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on b513d4386989:38847 (size: 7.1 KiB, free: 366.3 MiB)\n",
            "23/03/02 19:37:00 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1383\n",
            "23/03/02 19:37:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /content/word_count.py:14) (first 15 tasks are for partitions Vector(0))\n",
            "23/03/02 19:37:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "23/03/02 19:37:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (b513d4386989, executor driver, partition 0, PROCESS_LOCAL, 4485 bytes) taskResourceAssignments Map()\n",
            "23/03/02 19:37:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "23/03/02 19:37:01 INFO HadoopRDD: Input split: file:/content/wordcount.txt:0+4921\n",
            "23/03/02 19:37:02 INFO PythonRunner: Times: total = 714, boot = 648, init = 63, finish = 3\n",
            "23/03/02 19:37:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1654 bytes result sent to driver\n",
            "23/03/02 19:37:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1848 ms on b513d4386989 (executor driver) (1/1)\n",
            "23/03/02 19:37:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "23/03/02 19:37:02 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 48781\n",
            "23/03/02 19:37:02 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /content/word_count.py:14) finished in 2.152 s\n",
            "23/03/02 19:37:02 INFO DAGScheduler: looking for newly runnable stages\n",
            "23/03/02 19:37:02 INFO DAGScheduler: running: Set()\n",
            "23/03/02 19:37:02 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
            "23/03/02 19:37:02 INFO DAGScheduler: failed: Set()\n",
            "23/03/02 19:37:02 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "23/03/02 19:37:02 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 90.7 KiB, free 365.9 MiB)\n",
            "23/03/02 19:37:02 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 33.7 KiB, free 365.8 MiB)\n",
            "23/03/02 19:37:02 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on b513d4386989:38847 (size: 33.7 KiB, free: 366.2 MiB)\n",
            "23/03/02 19:37:02 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1383\n",
            "23/03/02 19:37:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "23/03/02 19:37:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "23/03/02 19:37:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (b513d4386989, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
            "23/03/02 19:37:02 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "23/03/02 19:37:02 INFO ShuffleBlockFetcherIterator: Getting 1 (3.5 KiB) non-empty blocks including 1 (3.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\n",
            "23/03/02 19:37:02 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\n",
            "23/03/02 19:37:02 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
            "23/03/02 19:37:02 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
            "23/03/02 19:37:02 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "23/03/02 19:37:02 INFO PythonRunner: Times: total = 51, boot = -669, init = 719, finish = 1\n",
            "23/03/02 19:37:02 INFO FileOutputCommitter: Saved output of task 'attempt_202303021936597540066217665317827_0008_m_000000_0' to file:/content/output/_temporary/0/task_202303021936597540066217665317827_0008_m_000000\n",
            "23/03/02 19:37:02 INFO SparkHadoopMapRedUtil: attempt_202303021936597540066217665317827_0008_m_000000_0: Committed\n",
            "23/03/02 19:37:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1952 bytes result sent to driver\n",
            "23/03/02 19:37:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 272 ms on b513d4386989 (executor driver) (1/1)\n",
            "23/03/02 19:37:02 INFO DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:83) finished in 0.329 s\n",
            "23/03/02 19:37:02 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "23/03/02 19:37:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "23/03/02 19:37:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "23/03/02 19:37:02 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:83, took 2.653840 s\n",
            "23/03/02 19:37:02 INFO SparkHadoopWriter: Job job_202303021936597540066217665317827_0008 committed.\n",
            "23/03/02 19:37:02 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "23/03/02 19:37:02 INFO SparkUI: Stopped Spark web UI at http://b513d4386989:4051\n",
            "23/03/02 19:37:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "23/03/02 19:37:02 INFO MemoryStore: MemoryStore cleared\n",
            "23/03/02 19:37:02 INFO BlockManager: BlockManager stopped\n",
            "23/03/02 19:37:02 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "23/03/02 19:37:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "23/03/02 19:37:02 INFO SparkContext: Successfully stopped SparkContext\n",
            "23/03/02 19:37:02 INFO ShutdownHookManager: Shutdown hook called\n",
            "23/03/02 19:37:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-84351251-e4c3-48b5-9989-79628508a218/pyspark-12cd5b88-5d5f-478a-991c-94f3ac6a26e2\n",
            "23/03/02 19:37:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-550b072a-9004-4d29-b975-fcc6bf89d444\n",
            "23/03/02 19:37:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-84351251-e4c3-48b5-9989-79628508a218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9Vgt01suSXl",
        "outputId": "ac8f278f-44a1-4160-d281-dd8e83ef9198"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cars.csv\t\t\tsample_data\n",
            "drive\t\t\t\tspark-3.1.1-bin-hadoop3.2\n",
            "ngrok\t\t\t\tspark-3.1.1-bin-hadoop3.2.tgz\n",
            "ngrok-stable-linux-amd64.zip\tword_count.py\n",
            "ngrok-stable-linux-amd64.zip.1\twordcount.txt\n",
            "ngrok-stable-linux-amd64.zip.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k_n50XFVxBXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}